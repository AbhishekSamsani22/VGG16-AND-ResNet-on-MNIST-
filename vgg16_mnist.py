# -*- coding: utf-8 -*-
"""VGG16_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14VPCCfyW1KLwBW96Xm_Qrcuzb24Pe1kY
"""

def mnist_architecture_VGG16(): 

    #importing the required libraries
    import time
    start_time = time.time()
    import tensorflow as tf
    from keras.datasets import mnist
    from keras.utils import np_utils
    from keras.models import Sequential
    from keras.layers.core import Dense, Dropout, Flatten
    from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D
    from keras.optimizers import RMSprop
    import matplotlib.pyplot as plt

    

    #Training Parameters
    BATCH_SIZE = 400; NB_EPOCH =75; NB_CLASSES = 10; VERBOSE = 1; VALIDATION_SPLIT = 0.2;

    #loading mnist_dataset
    (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()
    #checking the shape of the data
    print('X_train shape:', X_train.shape)
    print(X_train.shape[0], 'train samples')
    print(X_test.shape[0], 'test samples')


    #changing the shape of the output layer inorder to match with the dense layer
    y_train = np_utils.to_categorical(y_train, NB_CLASSES)
    y_test = np_utils.to_categorical(y_test, NB_CLASSES) 
    

    # float and normalization
    # Reshaping the array to 4-dims so that it can work with the Keras API
    X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)
    X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)
    X_train = X_train.astype('float32'); X_test = X_test.astype('float32');
    X_train /= 255; X_test /= 255;

    #VGG16 MODEL
       

    model = Sequential()
    
    model.add(ZeroPadding2D((1,1), input_shape=(28, 28, 1)))
    model.add(Conv2D(filters = 64, kernel_size = 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 64, kernel_size = 3, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 128, kernel_size = 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 128, kernel_size = 3, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 256, kernel_size = 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 256, kernel_size = 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 256, kernel_size = 3, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 512, kernel_size = 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 512, kernel_size = 3, activation='relu'))
    model.add(ZeroPadding2D((1,1)))
    model.add(Conv2D(filters = 512, kernel_size = 3, activation='relu'))
    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))

   
    # Flattening the 2D arrays for fully connected layers
    model.add(Flatten())

    #top layer of the VGG net
    model.add(Dense(units = 128, activation='relu')); model.add(Dropout(0.5))
    model.add(Dense(NB_CLASSES, activation='softmax'))
    model.summary()

    
    #COMPILING THE MODEL BY ADDING OPTIMIZER AND OBJECTIVE FUNCTION
    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(), metrics=['accuracy'])
    
    #TRAINING THE MODEL
    history = model.fit(X_train, y_train, batch_size=BATCH_SIZE,
           epochs=NB_EPOCH, validation_split=VALIDATION_SPLIT, verbose=VERBOSE)

    print('Testing...')
    score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=VERBOSE)
    print("\nTest score:", score[0]); print('Test accuracy:', score[1]);

    # summarize history for accuracy
    plt.plot(history.history['accuracy']); plt.plot(history.history['val_accuracy']);
    plt.title('model accuracy'); plt.ylabel('accuracy'); plt.xlabel('epoch'); 
    plt.legend(['train', 'test'], loc='upper left'); plt.show()

    # summarize history for loss
    plt.plot(history.history['loss']); plt.plot(history.history['val_loss']);
    plt.title('model loss'); plt.ylabel('loss'); plt.xlabel('epoch');
    plt.legend(['train', 'test'], loc='upper left'); plt.show()
    print(); print("Execution Time %s seconds: " % (time.time() - start_time))  

    # Saving The Model For future use
    model_json = model.to_json()
    open('mnist_architecture_VGG16.json', 'w').write(model_json)
    model.save_weights('mnist_weights_VGG16.h5', overwrite=True)

  
    # Testing our model on an image
    image_index = 622   #use this to test the image
    plt.imshow(X_test[image_index].reshape(28, 28),cmap='Greys')
    pred = model.predict(X_test[image_index].reshape(1, 28, 28, 1))
    print(pred.argmax())


     

mnist_architecture_VGG16()

